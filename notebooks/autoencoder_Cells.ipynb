{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to build our autoencoder implementation with PyTorch. PyTorch usually expect tensors for the implementations. Further, we want to do some standard preprocessing on the datasets. Which we already implemented with pandas. Therefore we need to convert these after preprocessing to tensors. Also, our idea is to work on the differences between good and bad cased. In datasets with a highly disproportionate ratio between these a systematic reconstruction error on the smaller subset would have less influence. Assuming the good cases in the datasets are always more, we want to undersample the good cases such that we have the same subset size between good and bad cases. To achieve all this, we build a dataload function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv file, woe encode categorical features, standardize values, make tensor with shape [n_rows, n_features]\n",
    "def load_data_to_tensor(dataset_name):\n",
    "    complete_data = pd.read_csv(f'../prepared_data/{dataset_name}', sep=',')\n",
    "    complete_data['BAD'] = np.where(complete_data['BAD'] == 'BAD', 1, 0).astype(np.int64)\n",
    "\n",
    "    # We down sample the GOOD datapoints to even out the imbalance in the data set\n",
    "    complete_data = pd.concat([complete_data[complete_data['BAD'] == 0].sample(complete_data['BAD'].value_counts()[1],random_state=37841), complete_data[complete_data['BAD'] == 1]])\n",
    "\n",
    "    # For the sake of simplicity when dealing with neural nets later, let's just make everything categorical continous\n",
    "    obj_cols = complete_data.select_dtypes('object').columns\n",
    "    complete_data[obj_cols] = complete_data[obj_cols].astype('category')\n",
    "\n",
    "    for col in obj_cols:\n",
    "        woe_calc = h.IV_Calc(complete_data, feature=col, target='BAD')\n",
    "        woe = woe_calc.full_summary()['WOE_adj'].to_dict()\n",
    "        complete_data[col] = complete_data[col].map(woe)\n",
    "        complete_data[col] = complete_data[col].astype('float64')\n",
    "\n",
    "    # Split the target variable from the rest of the dataset\n",
    "    complete_X = complete_data.iloc[:, complete_data.columns != 'BAD']\n",
    "    complete_y = complete_data['BAD']\n",
    "\n",
    "    x_np = complete_X.values.reshape(-1, complete_X.shape[1]).astype('float32')\n",
    "    y_np = complete_y.values.reshape(-1, 1).astype('float32')\n",
    "\n",
    "    # stadardize values\n",
    "    standardizer = StandardScaler()\n",
    "    x_stand = standardizer.fit_transform(x_np)\n",
    "\n",
    "    return torch.from_numpy(x_stand), torch.from_numpy(y_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used by our implementation of a dataset class which implements only the necessary functions getitem and length for the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class, that loads the standardized and prepared data from function load_data_to_tensor()\n",
    "class CreditscoringDataset(Dataset):\n",
    "    def __init__(self, dataset_name):\n",
    "        self.x, self.y = load_data_to_tensor(dataset_name)\n",
    "        \n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual autoencoder class takes a shape parameter to test different configurations of the layers. The shape parameter needs to be  indexable and contain integers with the expected sizes of the hidden layers. It warns if the last and the first layer of the shape aren't of the same size. This would make it difficult to compare the input and the output to minimize the construction error. We then stack linear layers with sizees from the first given size up to the smallest as the encoder. Similarly, the decoder is build by stacking linear layers with sizes from the smallest to last given value in the shape parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        if shape[0] != shape[-1]:\n",
    "            print('Warning! First and last layer of encoder do not have the same size.')\n",
    "\n",
    "        self.enc = nn.ModuleList()\n",
    "        self.dec = nn.ModuleList()\n",
    "\n",
    "        # Build encoder part\n",
    "        for i in range(shape.index(min(shape))):\n",
    "            self.enc.append(nn.Linear(in_features = shape[i], out_features = shape[i + 1]))\n",
    "        \n",
    "        # Build decoder part\n",
    "        for i in range(shape.index(min(shape)), len(shape) - 1):\n",
    "            self.dec.append(nn.Linear(in_features = shape[i], out_features = shape[i + 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class provides seperate encode and decode functions. This gives us the possibility to use only the encoder part on our dataset after the training on the neural network and train a credit scoring model based on the encoded data. The forward function is needed for the training of the neural network and just calls the encoder and afterwards decoder part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decode(self.encode(x))\n",
    "        return x\n",
    "\n",
    "    def encode(self, x):\n",
    "        for e in self.enc:\n",
    "            x = torch.tanh(e(x))\n",
    "        return x\n",
    "        \n",
    "    def decode(self, x):\n",
    "        for d in self.dec:\n",
    "            x = torch.tanh(d(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train any net\n",
    "def train(net, trainloader, epochs, learningrate, lossFuncWeights):\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion2 = nn.KLDivLoss(log_target=True, reduction=\"batchmean\")\n",
    "    criterion3 = mmd.MMD_loss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learningrate)\n",
    "\n",
    "    train_loss = []\n",
    "    train_loss_mmse = []\n",
    "    train_loss_mmd = []\n",
    "    train_loss_kld = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_loss_mmse = 0.0\n",
    "        running_loss_mmd = 0.0\n",
    "        running_loss_kld = 0.0\n",
    "        for data in trainloader:\n",
    "            data_x, data_y = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(data_x)\n",
    "            encoded = net.encode(data_x)\n",
    "\n",
    "            # split encoded data into good and bad subsets\n",
    "            good = [True if x == 0 else False for x in data_y]\n",
    "            enc_good = encoded[good]\n",
    "            enc_bad = encoded[[not value for value in good]]\n",
    "            #print(f'Enc_good shape: {enc_good.shape} Enc_bad shape: {enc_bad.shape}')\n",
    "\n",
    "            # build MultiNorm Distributions from subsets and create log_probs of enc_good for both distributions to compare with KLDIVLOSS\n",
    "            MN_dist_good = dis.multivariate_normal.MultivariateNormal(torch.mean(enc_good, dim=0), torch.corrcoef(torch.transpose(enc_good, 0, 1)))\n",
    "            MN_dist_bad = dis.multivariate_normal.MultivariateNormal(torch.mean(enc_bad, dim=0), torch.corrcoef(torch.transpose(enc_bad, 0, 1)))\n",
    "\n",
    "            sample = MN_dist_good.sample((1000,))\n",
    "\n",
    "            enc_good = enc_good[:min([len(enc_good),len(enc_bad)])]\n",
    "            enc_bad = enc_bad[:min([len(enc_good),len(enc_bad)])]\n",
    "\n",
    "            # calculate criterions only if they influence the overall loss\n",
    "            MMSELoss = criterion(outputs, data_x)                                                           if lossFuncWeights[0] > 0.0 else torch.zeros(1)\n",
    "            KLDivLoss = criterion2(MN_dist_bad.log_prob(sample), MN_dist_good.log_prob(sample)) * 1000000   if lossFuncWeights[1] > 0.0 else torch.zeros(1)\n",
    "            MMDLoss = criterion3(enc_good,enc_bad) * 10                                                     if lossFuncWeights[2] > 0.0 else torch.zeros(1)\n",
    "\n",
    "            loss = lossFuncWeights[0] * MMSELoss + lossFuncWeights[1] * KLDivLoss + lossFuncWeights[2] * MMDLoss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            running_loss_mmse += MMSELoss.item()\n",
    "            running_loss_mmd += MMDLoss.item()\n",
    "            running_loss_kld += KLDivLoss.item()\n",
    "\n",
    "        loss = running_loss / len(trainloader)\n",
    "        MMSELoss = running_loss_mmse / len(trainloader)\n",
    "        MMDLoss = running_loss_mmd / len(trainloader)\n",
    "        KLDivLoss = running_loss_kld / len(trainloader)\n",
    "        train_loss.append(loss)\n",
    "        train_loss_mmse.append(MMSELoss)\n",
    "        train_loss_mmd.append(MMDLoss)\n",
    "        train_loss_kld.append(KLDivLoss)\n",
    "        \n",
    "        #print('Epoch {} of {}, Train Loss: {:.4f} (MMSE: {:.4f} | MMD: {:.4f} | KLD: {:.4f})'.format(epoch+1, epochs, loss, MMSELoss, MMDLoss, KLDivLoss))\n",
    "\n",
    "    return train_loss, train_loss_mmse, train_loss_mmd, train_loss_kld"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27377a24656f16c7763d6e582fb7acd92211fc045094d5889878cd09fb47e40b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
